#show create table statement
SHOW CREATE TABLE myTable;



#export hive table to local file

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/temp.csv' ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
select * from user_jzile.pmp_sonob_ids_export limit 100;



# export hive table in gzip to local file
--extra options:
--set mapred.output.compress=true;
--set hive.exec.compress.output=true;
--set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;
--set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;

--must have otpions for compression
set hive.exec.compress.output=true;
set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;

--output to local /tmp/temp dir
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/temp' ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
select * from db1.tbl1;



#explode , make one filed from array to rows, can only explode one field, no other field can be select
#lateral view, used with explode to expand arry into multiple rows
#example: table1
#name, value_arr
#john, [1,2,3]
select name, dummy_values from table1 lateral view explode(value_arr) dummy as dummy_values;
#this prints out
#john,1
#john,2
#john,3
#in this example the value_arr is exploded and then converted lateral view [(john,1),(john,2), (john, 3)] as a new table, and needed to be selected in a new select statement
#

#hive create table
CREATE TABLE [IF NOT EXISTS] db_name.]table_name
  [PRIMARY KEY (col_name[, ...])]
  [PARTITION BY kudu_partition_clause
  [COMMENT 'table_comment']
  STORED AS KUDU
  [TBLPROPERTIES ('key1'='value1', 'key2'='value2', ...)]
AS
  select_statement



merge into customer_partitioned
 using all_updates on customer_partitioned.id = all_updates.id
 when matched then update set
   email=all_updates.email,
   state=all_updates.state
 when not matched then insert
   values(all_updates.id, all_updates.name, all_updates.email,
   all_updates.state, all_updates.signup);


# load data into a partition
LOAD DATA INPATH '{hdfs_file_to_load}' INTO TABLE table1 partition(partitionname1=partitionvalue1)

# load data and then overwrite the table
LOAD DATA INPATH 'hdfs_file_or_directory_path' [OVERWRITE] INTO TABLE tablename
  [PARTITION (partcol1=val1, partcol2=val2 ...)]

# output hive to a local file
# hive -f /sql/temp.hql | grep -v "WARN" | gzip > /tmp/file1


# copy a table schema in hive, duplicate a table, create table like
CREATE TABLE db1.tbl1
LIKE db2.tbl2


# hive set variable
hive> set CURRENT_DATE='2012-09-16';
hive> select * from foo where day >= ${hiveconf:CURRENT_DATE}
# hql-cli set variable
hive -hiveconf CURRENT_DATE='2012-09-16' -f test.hql
set hivevar:tablename=mytable;
hive> set tablename=newtable;
hive> select * from ${tablename} -- uses 'newtable'

# show hive partitions
show partitions table1

# loads field quoted csv file
CREATE TABLE Table(A varchar(50),B varchar(50),C varchar(50))
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
    "separatorChar" = ",",
    "quoteChar"     = "\""
)  
STORED AS TEXTFILE;
TBLPROPERTIES("skip.header.line.count"="1");


# select from another table as a new partition of a table
# seems this is wrong
INSERT INTO TABLE xxx partiton ( xxx ) SELECT xxx;
# below cmd works
INSERT OVERWRITE TABLE xxx partition(day='2019-09-12') SELECT ...;

# show partition of specific partition
show partitions db1.tbl1 partition(day='2019-07-30')

# ignore header
tblproperties ("skip.header.line.count"="1");

# get sha256 of a string
reflect('org.apache.commons.codec.digest.DigestUtils', 'sha256Hex', column1)

# aggregate multiple rows value into array
select 
    User, 
    collect_set(Alias) as Alias
from table
group by User;

# alter table to add more column
ALTER TABLE new_table ADD COLUMNS (newCol1 int,newCol2 int);

# hive select all but 1 column
# Try to setup the below property
set hive.support.quoted.identifiers=none;
# Then select all columns except col_21:
select `(col_21)?+.+` from <table_name>; 


# join on multiple columns
from a join b on (a.col1=b.col1 and a.col2=b.col2)

# rlike in hive, multiple like clause or 
and col1 RLIKE '^str1*|.*str2.*' ;

# cast to int
cast(str_column as int)

# hive in
col1 in (val1, val2)

# hive wild card join, hive doesn't support wild card join, but we can use like to join two table in where clause

# hive over partition by, this claus is to use display individual element together with some aggregated element
# in below case, it is id,name,gender together with count(gender) from all student
SELECT id, name, gender,
COUNT(gender) OVER (PARTITION BY gender) AS Total_students,
AVG(age) OVER (PARTITION BY gender) AS Average_Age,
SUM(total_score) OVER (PARTITION BY gender) AS Total_Score
FROM student


# below is an example of over partition oby
# id, group, starttime 3 columns, now we add a rank to it which ordered by timestamp and over partition by name(means group by name), so in each group there will always be a record have rank=1
select * from (select id, group, starttime, rank() over(partition by name order by unix_timestamp(starttime, 'EEE, dd MMM yyyy hh:mm:ss z') desc) as rnk from hive_table) a where a.rnk=1;


# drop a partition
ALTER TABLE logs DROP IF EXISTS PARTITION(year = 2012, month = 12, day = 18);


# change order of column in hive
alter table db1.tbl1 change col1 col1 string after col3;

# calculate percentage in hive
SELECT t1.colB,
  SUM( IF( colC == ‘y’, 1 , 0 ) )/ COUNT() 100 as pct
  FROM tbl t1
  GROUP BY t1.colB;

# row_number() just like rank() will assign one extra column number into result set
row_number() over (partition by userid, segmentid order by userid, segmentid) as rownumber

# get create table statment
SHOW CREATE TABLE myTable;


#hive looks like only mounts data and when select adds partition key, so a data file can be copied around



# insert records
insert into table tbl1(col1) values('col1_val');


# What is the biggest difference between Hive and Impala?
they have different engine
hive compiles into map reduce job
and impala has its own mpp engine and converts big loops into actual code

hive saves intermediateresult to file which make scaling easier
impala streaming intermediate result, faster

# Why would you want to use Hive?
hive is good in ETL job and complex query
# Why would you want to use Impala? When can you *not* use Impala?
impala caches query, so it is fast. if you are a data analyst, you want to use impala, only case not use impala is some formating are not supported in impala


# tweak hive to use more mapper
set hive.merge.mapfiles=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; # we can change it to smaller but it would end up to have too many overhead to reduce efficiency
set mapred.map.tasks = XX;

# also you can tweak it smaller by increasing the file split size
set mapreduce.input.fileinputformat.split.maxsize=512000000; # default is 256

# example of tweaking to speed up a single reducer job
writing query in hive like this:

 SELECT COUNT(DISTINCT id) ....
 will always result in using only one reducer. You should:

 use this command to set desired number of reducers:

 set mapred.reduce.tasks=50

 rewrite query as following:

 SELECT COUNT(*) FROM ( SELECT DISTINCT id FROM ... ) t;

 This will result in 2 map+reduce jobs instead of one, but performance gain will be substantial.
