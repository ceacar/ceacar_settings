CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name

[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[ROW FORMAT row_format]
[STORED AS file_format]


#show create table statement
SHOW CREATE TABLE myTable;



#export hive table to local file

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/temp.csv' ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
select * from user_jzile.pmp_sonob_ids_export limit 100;



# export hive table in gzip to local file
--extra options:
--set mapred.output.compress=true;
--set hive.exec.compress.output=true;
--set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;
--set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;

--must have otpions for compression
set hive.exec.compress.output=true;
set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;

--output to local /tmp/temp dir
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/temp' ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
select * from db1.tbl1;



#explode , make one filed from array to rows, can only explode one field, no other field can be select
#lateral view, used with explode to expand arry into multiple rows
#example: table1
#name, value_arr
#john, [1,2,3]
select name, dummy_values from table1 lateral view explode(value_arr) dummy as dummy_values;
#this prints out
#john,1
#john,2
#john,3
#in this example the value_arr is exploded and then converted lateral view [(john,1),(john,2), (john, 3)] as a new table, and needed to be selected in a new select statement
#

#hive create table
CREATE TABLE [IF NOT EXISTS] db_name.]table_name
  [PRIMARY KEY (col_name[, ...])]
  [PARTITION BY kudu_partition_clause
  [COMMENT 'table_comment']
  STORED AS KUDU
  [TBLPROPERTIES ('key1'='value1', 'key2'='value2', ...)]
AS
  select_statement



merge into customer_partitioned
 using all_updates on customer_partitioned.id = all_updates.id
 when matched then update set
   email=all_updates.email,
   state=all_updates.state
 when not matched then insert
   values(all_updates.id, all_updates.name, all_updates.email,
   all_updates.state, all_updates.signup);


# load data into a partition
LOAD DATA INPATH '{hdfs_file_to_load}' INTO TABLE table1 partition(partitionname1=partitionvalue1)

# load data and then overwrite the table
LOAD DATA INPATH 'hdfs_file_or_directory_path' [OVERWRITE] INTO TABLE tablename
  [PARTITION (partcol1=val1, partcol2=val2 ...)]

# output hive to a local file
# hive -f /sql/temp.hql | grep -v "WARN" | gzip > /tmp/file1


# copy a table schema in hive, duplicate a table, create table like
CREATE TABLE db1.tbl1
LIKE db2.tbl2


# hive set variable
hive> set CURRENT_DATE='2012-09-16';
hive> select * from foo where day >= ${hiveconf:CURRENT_DATE}
# hql-cli set variable
hive -hiveconf CURRENT_DATE='2012-09-16' -f test.hql
set hivevar:tablename=mytable;
hive> set tablename=newtable;
hive> select * from ${tablename} -- uses 'newtable'

# show hive partitions
show partitions table1

# loads field quoted csv file
CREATE TABLE Table(A varchar(50),B varchar(50),C varchar(50))
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES 
(
    "separatorChar" = ",",
    "quoteChar"     = "\""
)  
STORED AS TEXTFILE;
TBLPROPERTIES("skip.header.line.count"="1");


# select from another table as a new partition of a table
# seems this is wrong
INSERT INTO TABLE xxx partiton ( xxx ) SELECT xxx;
# below cmd works
INSERT OVERWRITE TABLE xxx partition(day='2019-09-12') SELECT ...;

# show partition of specific partition
show partitions db1.tbl1 partition(day='2019-07-30')

# ignore header
tblproperties ("skip.header.line.count"="1");

# get sha256 of a string
reflect('org.apache.commons.codec.digest.DigestUtils', 'sha256Hex', column1)

# aggregate multiple rows value into array
select 
    User, 
    collect_set(Alias) as Alias
from table
group by User;

# alter table to add more column, add column
ALTER TABLE new_table ADD COLUMNS (newCol1 int,newCol2 int);
# change an already partitions table to add column, this will apply to all partitions
ALTER TABLE default.test_table ADD columns (column1 string,column2 string) CASCADE;

# add column in the middle of columns

    # In your case, first add the column user_id to the table with below command:
    ALTER TABLE table_name ADD COLUMNS (user_id BIGINT);
    # Now to make user_id column as the first column in your table use change column with FIRST clause:
    ALTER TABLE table_name CHANGE COLUMN user_id user_id BIGINT first; # This will move the user_id column to the first position.

    # Similarly you can use After instead of first if you want to move the specified column after any other column. Like say, I want to move dob column after user_id column. Then my command would be:
    ALTER TABLE table_name CHANGE COLUMN dob dob date AFTER user_id; #this one only change the future partitions;
    ALTER TABLE table_name CHANGE COLUMN dob dob date AFTER user_id CASCADE; # this looks like to add a null column at the end and then add a null value after the user_id for old data, so a table migration is needed

# hive select all but 1 column
# Try to setup the below property
set hive.support.quoted.identifiers=none;
# Then select all columns except col_21:
select `(col_21)?+.+` from <table_name>; 


# join on multiple columns
from a join b on (a.col1=b.col1 and a.col2=b.col2)

# rlike in hive, multiple like clause or 
and col1 RLIKE '^str1*|.*str2.*' ;

# cast to int
cast(str_column as int)

# hive in
col1 in (val1, val2)

# hive wild card join, hive doesn't support wild card join, but we can use like to join two table in where clause

# hive over partition by, this claus is to use display individual element together with some aggregated element
# in below case, it is id,name,gender together with count(gender) from all student
SELECT id, name, gender,
COUNT(gender) OVER (PARTITION BY gender) AS Total_students,
AVG(age) OVER (PARTITION BY gender) AS Average_Age,
SUM(total_score) OVER (PARTITION BY gender) AS Total_Score
FROM student


# below is an example of over partition oby
# id, group, starttime 3 columns, now we add a rank to it which ordered by timestamp and over partition by name(means group by name), so in each group there will always be a record have rank=1
select * from (select id, group, starttime, rank() over(partition by name order by unix_timestamp(starttime, 'EEE, dd MMM yyyy hh:mm:ss z') desc) as rnk from hive_table) a where a.rnk=1;


# drop a partition
ALTER TABLE logs DROP IF EXISTS PARTITION(year = 2012, month = 12, day = 18);


# change order of column in hive
alter table db1.tbl1 change col1 col1 string after col3;

# calculate percentage in hive
SELECT t1.colB,
  SUM( IF( colC == ‘y’, 1 , 0 ) )/ COUNT() 100 as pct
  FROM tbl t1
  GROUP BY t1.colB;

# row_number() just like rank() will assign one extra column number into result set
row_number() over (partition by userid, segmentid order by userid, segmentid) as rownumber

# get create table statment
SHOW CREATE TABLE myTable;


#hive looks like only mounts data and when select adds partition key, so a data file can be copied around



# insert records
insert into table tbl1(col1) values('col1_val');


# What is the biggest difference between Hive and Impala?
they have different engine
hive compiles into map reduce job
and impala has its own mpp engine and converts big loops into actual code

hive saves intermediateresult to file which make scaling easier
impala streaming intermediate result, faster

# Why would you want to use Hive?
hive is good in ETL job and complex query
# Why would you want to use Impala? When can you *not* use Impala?
impala caches query, so it is fast. if you are a data analyst, you want to use impala, only case not use impala is some formating are not supported in impala


# tweak hive to use more mapper
set hive.merge.mapfiles=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat; # we can change it to smaller but it would end up to have too many overhead to reduce efficiency
set mapred.map.tasks = XX;

# also you can tweak it smaller by increasing the file split size
set mapreduce.input.fileinputformat.split.maxsize=512000000; # default is 256

# example of tweaking to speed up a single reducer job
writing query in hive like this:

 SELECT COUNT(DISTINCT id) ....
 will always result in using only one reducer. You should:

 use this command to set desired number of reducers:

 set mapred.reduce.tasks=50

 rewrite query as following:

 SELECT COUNT(*) FROM ( SELECT DISTINCT id FROM ... ) t;

 This will result in 2 map+reduce jobs instead of one, but performance gain will be substantial.

 # show tables in a database
 show tables in db1

 # trim string
 TRIM(" 1 ")  # this will return 1 which is free of spaces


# show describe statement in more details like location of the table
describe formatted table1
describe extended table1  # this shows more detail
describe formatted [db_name.]table_name column_name partition (partition_spec);



# static partitioning vs dynamic partitioning
# basically dynamic partitioning enables you to load data without specifying partition column value
# in static partitioning we need to specify the partition column value in each and every LOAD statement.
# suppose we are having partition on column country for table t1(userid, name,occupation, country), so each time we need to provide country value
hive>LOAD DATA INPATH '/hdfs path of the file' INTO TABLE t1 PARTITION(country="US")
hive>LOAD DATA INPATH '/hdfs path of the file' INTO TABLE t1 PARTITION(country="UK")

# dynamic partition allow us not to specify partition column value each time. the approach we follows is as below:
# create a non-partitioned table t2 and insert data into it.
# now create a table t1 partitioned on intended column(say country).
# load data in t1 from t2 as below:
hive.exec.dynamic.partition=true
hive.exec.dynamic.partition.mode=nostrict
> INSERT INTO TABLE t2 PARTITION(country) SELECT * from T1;
# make sure that partitioned column is always the last one in non partitioned table(as we are having country column in t2)


# Allow snapshots on a directory
hdfs dfsadmin –allowSnapshot <path>
# Reset a snapshottable directory
hdfs dfsadmin –disallowSnapshot <path> 

# Create/delete/rename snapshots
hdfs dfs -createSnapshot <path> [<snapshotName>]
hdfs dfs –deleteSnapshot <path> <snapshotName>
hdfs dfs –renameSnapshot <path> <oldName> <newName>
# Get snapshottable directory listing
hdfs lsSnapshottableDir
# Get snapshots difference report
hdfs snapshotDiff <path> <from> <to>


# All regular commands and APIs can be used against snapshot path
/<snapshottableDir>/.snapshot/<snapshotName>/foo/bar
# List all the files in a snapshot
ls /test/.snapshot/s4
# List all the snapshots under that path
ls <path>/.snapshot 

# drop partition and destroy data
ALTER TABLE {database}.{table} DROP PARTITION {partition_spec} PURGE

-- list tables in a database
show tables in dbname1;

-- show location of a partition 
show table extended like 'tbl_name' partition (dt='20131023');

# hive if else
CASE
  WHEN (condition1) THEN result1
  WHEN (condition2) THEN result2
  WHEN (condition3) THEN result3 
  WHEN (condition4) THEN result4
  ELSE result_default 
END AS attribute_name


ALTER TABLE REPLACE COLUMNS(col1 col1_type,...); # this will change column in a table


-- insert record in hive hive table
INSERT INTO TABLE foo
SELECT '12', 'xyz'
FROM t
LIMIT 1;


-- coalesce, this function provider a situition when we only want to take first non null value of a few columns, or we want put a default value for a column when it is null
COALESCE(field1, field2, … , fieldn)

-- disable compression
set session gridhive.compression_codec='NONE';


CREATE TABLE imps_part (
    id  INT,
    user_id    String,
    user_lang      STRING,
    user_device        STRING,
    time_stamp     String,
    url    String
)
PARTITIONED BY (date STRING, country String)
row format delimited 
fields terminated by ',' 
stored as textfile;


-- add partition to table with specific location

2
3
ALTER TABLE <Table_Name> ADD 
[IF NOT EXISTS] PARTITION 
<partition_spec> LOCATION 'hdfs_path_of_directory'



-- hive in exists can be rewritten to semi join
SELECT a.key, a.value
  FROM a
  WHERE a.key in
   (SELECT b.key
    FROM B);
--can be rewritten to:
 SELECT a.key, a.val
 FROM a LEFT SEMI JOIN b on (a.key = b.key)

-- when in subquery the field should have table name with it like this c.state
where
     lower(c.state) in (
   select state from onboard.states
)


-- hash a hive row
reflect('org.apache.commons.codec.digest.DigestUtils', 'sha256Hex', col1)
-- substr
substr(b.provider_business_mailing_address_postal_code, 1, 5)

--strright substitute
substr(work_zip, length(work_zip)+1 -4, 4)
