# define a dag
from airflow import models as af_models
Dag = af_models.DAG(
  dag_id = 'my_dag',
  start_date = datetime(2017,6,30),
  schedule_interval = '0 10 * * *'
  )

# useful args for DAG
# 1. default_args->the args DAG would feed to every task, handy when there are many common args across all tasks
# 2. max_active_task: the tasks count airflow can run
# 3. concurrency: the max tasks count can run concurrently

# define a task
import airflow.operators as af_op

first_task = af_op.PythonOperator(
  task_id = 'my_task1',
  python_callable = module.function,
  dag = DAG
)

second_task = af_op.PythonOperator(
  task_id = 'my_task2',
  python_callable = module.function,
  dag = DAG
)

# useful args for task
# 1. retries
# 2. pool: max concurrency tasks can run
# 3. queue : Parameters for only celery, assign specific worker
# 4. execution_time: timeout
# 5. trigger_rule: which task gets executed, for example-> some task failed, then run this task
# 6. Args for python callbacks: 
# 7. Environmental Variables
# 8. Template Variables: Ginger templates??

# set first_task as dependency of second_task
second_task.set_upstream(first_task)

# CeleryExecutor: airflow use it
# SequentialExecutor: have task run in same process as schduler, useful in debug mode, this mode can use pdb(amazing!)
# LocalExecutor: use python multiprocessing module on local host
# MesosExecutor: 

# Debugging:
# 1.SequentialExecutor
# 2.pdb
# 3.airflow test: no db task will ran, not sure??

# Local PipeLine Testing:
# 1. start_date=some_date
# 2. schedule_interval='@once': airflow only run once, will not do any backfilling despite of the start_date being set to the past
# 3. delete DAG run(DAG run is a file that indicate this pipeline has been ran) or task-instances to rerun

# Misc:
# Execution date: the date of previous task started, not current task start time

# Airflow CLI
# airflow scheduler --num_runs : this num_runs is the count of pipeline ran before airflow kills itself( have a while loop starting it could make airflow automatically loads the newest code, set it to 1 can make sure it will run once and then reload the code)
# CELERYD_MAX_TASKS_PER_CHILD: 



# video link
# https://www.youtube.com/watch?v=cHATHSB_450



# run airflow task, airflow run task_name dag_id execution_date
# dag_id can be fonud inside code
airflow run example_bash_operator runme_0 2015-01-01


# how to install airflow
# airflow needs a home, ~/airflow is the default,
# but you can lay foundation somewhere else if you prefer
# (optional)
export AIRFLOW_HOME=~/airflow
# install from pypi using pip
pip install apache-airflow
# initialize the database
airflow initdb
# start the web server, default port is 8080
airflow webserver -p 8080
# start the scheduler
airflow scheduler
# visit localhost:8080 in the browser and enable the example dag in the home page

# example cmd
# run your first task instance
airflow run example_bash_operator runme_0 2015-01-01
# # run a backfill over 2 days
airflow backfill example_bash_operator -s 2015-01-01 -e 2015-01-02

# print the list of active DAGs
airflow list_dags
# # prints the list of tasks the "tutorial" dag_id
airflow list_tasks tutorial
# # prints the hierarchy of tasks in the tutorial DAG
airflow list_tasks tutorial --tree


# command layout: command subcommand dag_id task_id date
# testing print_date
airflow test tutorial print_date 2015-06-01
# testing sleep
airflow test tutorial sleep 2015-06-01


# optional, start a web server in debug mode in the background
airflow webserver --debug &

# start your backfill on a date range
airflow backfill tutorial -s 2015-06-01 -e 2015-06-07

#clear run record of airflow, so the task can be rerun
airflow clear your_dag_id --task_regex task_B --start_date 2017-09-25T17:00:00 --end_date 2017-09-25T17:00:00

# airflow task dependecies
a >> b >> c # b runs before c and a runs before b
b << a # a runs before b


# airflow webserver checks /tmp by default to store cache files, we can rewrite the source code to look into somehwere else like termux ~/temp
#  File "/data/data/com.termux/files/home/.venv/my_python_venv3/lib/python3.7/site-packages/airflow/www/app.py", line 74, in create_app
#      cache = Cache(app=app, config={'CACHE_TYPE': 'filesystem', 'CACHE_DIR': '/tmp'})


# some good explanation of parallelism of airflow
parallelism: not a very descriptive name. The description says it sets the maximum task instances for the airflow installation, which is a bit ambiguous â€” if I have two hosts running airflow workers, I'd have airflow installed on two hosts, so that should be two installations, but based on context 'per installation' here means 'per Airflow state database'. I'd name this max_active_tasks.

dag_concurrency: Despite the name based on the comment this is actually the task concurrency, and it's per worker. I'd name this max_active_tasks_for_worker (per_worker would suggest that it's a global setting for workers, but I think you can have workers with different values set for this).

max_active_runs_per_dag: This one's kinda alright, but since it seems to be just a default value for the matching DAG kwarg, it might be nice to reflect that in the name, something like default_max_active_runs_for_dags So let's move on to the DAG kwargs:

concurrency: Again, having a general name like this, coupled with the fact that concurrency is used for something different elsewhere makes this pretty confusing. I'd call this max_active_tasks.

max_active_runs: This one sounds alright to me.
